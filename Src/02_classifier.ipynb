{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ed841f58",
   "metadata": {},
   "source": [
    "## test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a09233d9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import os\n",
    "\n",
    "# Set the CUDA device\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "\n",
    "# Check TensorFlow version and available GPU devices\n",
    "print(tf.__version__)\n",
    "print(tf.config.list_physical_devices('GPU'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0f6ee32",
   "metadata": {},
   "source": [
    "## import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "935519f7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "from tensorflow.keras.preprocessing import image\n",
    "from tensorflow.keras.layers import Input, Dense, Dropout, GlobalAveragePooling2D, Convolution2D, Flatten\n",
    "from tensorflow.keras.layers import MaxPooling2D, AveragePooling2D, BatchNormalization\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "from tensorflow.keras.models import load_model, Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14f3480c",
   "metadata": {},
   "source": [
    "## lable preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abf7d8f2-9f35-4950-b330-cf9874084234",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Read the training, validation, and test datasets from CSV files\n",
    "tr_df = pd.read_csv('F:/poorya/datasets/ThermalFaceDatabase/X_train.csv')\n",
    "vl_df = pd.read_csv('F:/poorya/datasets/ThermalFaceDatabase/X_val.csv')\n",
    "ts_df = pd.read_csv('F:/poorya/datasets/ThermalFaceDatabase/X_test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e985e318-d5b8-476b-858f-b2e61e1e7631",
   "metadata": {},
   "outputs": [],
   "source": [
    "def df_to_ary(df):\n",
    "    '''\n",
    "    Convert Pandas DataFrame columns 'x' and 'y' to a NumPy array.\n",
    "\n",
    "    Arguments:\n",
    "    df (pandas.DataFrame): Input DataFrame containing 'x' and 'y' columns.\n",
    "\n",
    "    Returns:\n",
    "    numpy.ndarray: NumPy array with transformed values from 'x' and 'y' columns.\n",
    "    '''\n",
    "    ary = np.zeros((len(df), 136), dtype = int)\n",
    "\n",
    "    for i in range(len(df)):\n",
    "        # Identify comma positions in 'x' and 'y' strings\n",
    "        rep_x = [j for j in range(len(df['x'][i])) if df['x'][i].startswith(',', j)]\n",
    "        rep_y = [j for j in range(len(df['y'][i])) if df['y'][i].startswith(',', j)]\n",
    "\n",
    "        # Extract and store x, y coordinates into the array\n",
    "        ary[i, 0] = int(df['x'][i][1:rep_x[0]])\n",
    "        ary[i, 67] = int(df['x'][i][rep_x[-1] + 1:-1])\n",
    "        ary[i, 68] = int(df['y'][i][1:rep_y[0]]) - 128\n",
    "        ary[i, 135] = int(df['y'][i][rep_y[-1] + 1:-1]) - 128\n",
    "\n",
    "        for k in range(66):\n",
    "            ary[i, k + 1] = int(df['x'][i][rep_x[k] + 1:rep_x[k + 1]])\n",
    "            ary[i, k + 69] = int(df['y'][i][rep_y[k] + 1:rep_y[k + 1]]) - 128\n",
    "\n",
    "    return ary\n",
    "\n",
    "\n",
    "# Convert DataFrame columns 'x' and 'y' to NumPy arrays\n",
    "tr_ary = df_to_ary(tr_df)  # Training data\n",
    "ts_ary = df_to_ary(ts_df)  # Test data\n",
    "vl_ary = df_to_ary(vl_df)  # Validation data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd75ece5",
   "metadata": {},
   "source": [
    "## functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "573ad701-af6b-4f39-8075-49fb81b1348e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataGenerator(tf.keras.utils.Sequence):\n",
    "    def __init__(self, df, ary, batch_size):\n",
    "        '''\n",
    "        Initializes a DataGenerator object.\n",
    "\n",
    "        Arguments:\n",
    "        df (pandas.DataFrame): DataFrame containing image paths.\n",
    "        ary (numpy.ndarray): Array containing keypoints data.\n",
    "        batch_size (int): Size of the batches for data generation.\n",
    "\n",
    "        Returns:\n",
    "        None\n",
    "        '''\n",
    "        self.df = df\n",
    "        self.ary = ary\n",
    "        self.batch_size = batch_size\n",
    "        self.n = df['images'].tolist()\n",
    "        self.on_epoch_end()\n",
    "\n",
    "    def __len__(self):\n",
    "        '''\n",
    "        Calculates the number of batches per epoch.\n",
    "\n",
    "        Arguments:\n",
    "        None\n",
    "\n",
    "        Returns:\n",
    "        int: Number of batches per epoch.\n",
    "        '''\n",
    "        return int(np.floor(len(self.n) / self.batch_size))\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        '''\n",
    "        Generates a batch of data.\n",
    "\n",
    "        Arguments:\n",
    "        index (int): Index of the current batch.\n",
    "\n",
    "        Returns:\n",
    "        tuple: Data batch (images, keypoint).\n",
    "        '''\n",
    "        indexes = self.indexes[index * self.batch_size:(index + 1) * self.batch_size]\n",
    "        list_IDs_temp = [self.n[k] for k in indexes]\n",
    "        x, y = self.__data_generation(list_IDs_temp, indexes)\n",
    "        return x, y\n",
    "\n",
    "    def on_epoch_end(self):\n",
    "        '''\n",
    "        Updates indexes after each epoch.\n",
    "\n",
    "        Arguments:\n",
    "        None\n",
    "\n",
    "        Returns:\n",
    "        None\n",
    "        '''\n",
    "        self.indexes = np.arange(len(self.n))\n",
    "\n",
    "    def __data_generation(self, list_IDs_temp, indexes):\n",
    "        '''\n",
    "        Generates data containing batch_size samples.\n",
    "\n",
    "        Arguments:\n",
    "        list_IDs_temp (list): List of image paths.\n",
    "        indexes (numpy.ndarray): Indexes of the images.\n",
    "\n",
    "        Returns:\n",
    "        tuple: Batch of images and keypoints data.\n",
    "        '''\n",
    "        images = np.empty((self.batch_size, 768, 768, 1))\n",
    "        keypoint = np.empty((self.batch_size, 136), dtype = float)\n",
    "\n",
    "        for i, ID in enumerate(list_IDs_temp):\n",
    "            # Load and preprocess the image\n",
    "            train_img = image.img_to_array(image.load_img('F:/poorya/datasets/ThermalFaceDatabase/' + ID, color_mode = \"grayscale\"))\n",
    "            train_img /= 255.0\n",
    "            # Crop the image and assign it to the images array\n",
    "            images[i,] = train_img[:, 128:896]\n",
    "            # Assign keypoints to the keypoint array\n",
    "            keypoint[i,] = self.ary[int(indexes[i]), :] / 768\n",
    "\n",
    "        return images, keypoint\n",
    "\n",
    "\n",
    "# Batch size\n",
    "bs = 4\n",
    "# Create DataGenerator instances for training, validation, and test data\n",
    "train_gen = DataGenerator(tr_df, tr_ary, bs)\n",
    "val_gen = DataGenerator(vl_df, vl_ary, bs)\n",
    "test_gen = DataGenerator(ts_df, ts_ary, bs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c64186d3",
   "metadata": {},
   "source": [
    "## model structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01698c1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model():\n",
    "    '''\n",
    "    Generates a new model by utilizing transfer learning and adding custom layers.\n",
    "\n",
    "    Returns:\n",
    "    tf.keras.Model: The customized model for the specified task.\n",
    "    '''\n",
    "    # Initialization\n",
    "    drp = 0.0\n",
    "    act = 'relu'\n",
    "\n",
    "    # Backbone\n",
    "    img_input = Input(shape = (768, 768, 1))\n",
    "\n",
    "    # Load pre-trained model and freeze layers\n",
    "    full_model = load_model('checkpoint')\n",
    "    model = Model(inputs = full_model.inputs, outputs = full_model.layers[29].output)\n",
    "    model.trainable = False\n",
    "\n",
    "    # Connect to the flattened output\n",
    "    x = model(img_input)\n",
    "\n",
    "    # Additional Dense layers\n",
    "    x = Flatten()(x)\n",
    "    x = Dense(1024, activation = act)(x)\n",
    "    x = Dropout(drp)(x)\n",
    "    x = Dense(512, activation = act)(x)\n",
    "    x = Dropout(drp)(x)\n",
    "    x = Dense(256, activation = act)(x)\n",
    "    x = Dropout(drp)(x)\n",
    "\n",
    "    # Final output layer with sigmoid activation\n",
    "    last = Dense(136, activation = 'sigmoid')(x)\n",
    "\n",
    "    return Model(inputs = img_input, outputs = last)\n",
    "\n",
    "\n",
    "# Define optimizer, loss, and metrics\n",
    "metrics = ['accuracy']\n",
    "opt = tf.keras.optimizers.Adamax(learning_rate = 0.0001)\n",
    "loss = tf.keras.losses.MeanSquaredError()\n",
    "\n",
    "# Compile the model and summarize its architecture\n",
    "model = get_model()\n",
    "model.compile(loss = loss, optimizer = opt, metrics = [tf.keras.metrics.MeanAbsolutePercentageError()])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21459a3f",
   "metadata": {},
   "source": [
    "## train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51b075ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tr():\n",
    "    '''\n",
    "    Generates a list of callbacks to use during model training.\n",
    "\n",
    "    Returns:\n",
    "    list: List of selected callbacks.\n",
    "    '''\n",
    "    # Define different callbacks for model training\n",
    "    chk = ModelCheckpoint(filepath = 'keypoint final', save_format = \"h5\", monitor = 'val_loss', mode = 'min', verbose = 1, save_best_only = True)\n",
    "    ers = EarlyStopping(monitor = 'val_loss', mode = 'min', verbose = 1, patience = 75)\n",
    "    rlr = tf.keras.callbacks.ReduceLROnPlateau(monitor = 'val_loss', factor = 0.8, patience = 5, min_lr = 0.00001)\n",
    "\n",
    "    # Return a list of selected callbacks (in this case, only the ModelCheckpoint)\n",
    "    return [chk]\n",
    "\n",
    "\n",
    "# Obtain the callbacks list by calling the function 'tr()'\n",
    "cll = tr()\n",
    "\n",
    "# Fit the model using the generated callbacks along with other parameters\n",
    "history = model.fit(\n",
    "    train_gen,\n",
    "    validation_data = val_gen,\n",
    "    batch_size = bs,\n",
    "    epochs = 100,\n",
    "    verbose = 1,\n",
    "    callbacks = cll,  # Using the obtained callbacks\n",
    "    steps_per_epoch = np.shape(tr_ary)[0] // bs,\n",
    "    validation_steps = np.shape(vl_ary)[0] // bs\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94bcc323",
   "metadata": {},
   "source": [
    "## evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02a3172d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieve loss and validation loss data from the history object\n",
    "training_loss = history.history['loss']\n",
    "validation_loss = history.history['val_loss']\n",
    "\n",
    "# Plot the data\n",
    "plt.plot(training_loss, label = 'Training Loss')\n",
    "plt.plot(validation_loss, label = 'Validation Loss')\n",
    "\n",
    "# Adding title and labels to the plot\n",
    "plt.title('Training and Validation Loss per Epoch')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "\n",
    "# Display legend and show the plot\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ff3dab6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating an empty image\n",
    "img = np.zeros((1, 768, 768, 1)).astype('double')\n",
    "\n",
    "# Loading an image using its path from the DataFrame 'tr_df'\n",
    "train_igg = image.load_img('F:/poorya/datasets/ThermalFaceDatabase/' + tr_df['images'][40], color_mode = \"grayscale\")\n",
    "train_img = image.img_to_array(train_igg)\n",
    "train_img /= 255.0\n",
    "\n",
    "# Cropping the image and assigning it to 'img'\n",
    "img[0, :, :, :] = train_img[:, 128:896]\n",
    "img = np.array(img)\n",
    "\n",
    "# Loading the pre-trained model for predicting keypoints\n",
    "classifier = load_model('keypoint final')\n",
    "lbl_c = classifier.predict(img)\n",
    "\n",
    "# Extracting predicted keypoints for x and y axes\n",
    "x = lbl_c[0, 0:68] * 768\n",
    "y = lbl_c[0, 68:] * 768\n",
    "\n",
    "# Plotting the image with the predicted keypoints\n",
    "plt.figure(figsize = (20, 10))\n",
    "plt.imshow(img[0], cmap = 'gray')\n",
    "plt.scatter(y, x)  # Plotting the keypoints\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9812021",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Calculate loss and metric for test set\n",
    "tst_loss, tst_acc = classifier.evaluate(test_gen, steps = np.shape(ts_ary)[0] // bs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb7b3823",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the list of image paths from the DataFrame\n",
    "n = ts_df['images'].tolist()\n",
    "\n",
    "# Create an array to store processed images\n",
    "x = np.zeros((len(n), 768, 768, 1))\n",
    "\n",
    "# Process each image and store it in the 'x' array\n",
    "for i in range(len(n)):\n",
    "    train_img = image.img_to_array(image.load_img('F:/poorya/datasets/ThermalFaceDatabase/' + n[\n",
    "        i], color_mode = \"grayscale\"))\n",
    "    train_img /= 255.0\n",
    "    x[i] = train_img[:, 128:896]\n",
    "\n",
    "# Make predictions using the 'classifier' model for the test images\n",
    "pred = classifier.predict(x)\n",
    "\n",
    "# Calculate the Normalized Mean Error (NME)\n",
    "summ = 0\n",
    "for i in range(np.shape(pred)[0]):\n",
    "    y_pred = pred[i] * 768  # Scaling the predicted keypoints\n",
    "    y_true = ts_ary[i]  # Ground truth keypoints\n",
    "\n",
    "    # Calculate NME for each keypoint pair (x, y)\n",
    "    x1 = y_pred[42:48].mean()\n",
    "    x2 = y_pred[36:42].mean()\n",
    "    y1 = y_pred[42 + 68:48 + 68].mean()\n",
    "    y2 = y_pred[36 + 68:42 + 68].mean()\n",
    "\n",
    "    # Calculate NME for the current image and update the summation\n",
    "    summ += np.linalg.norm(y_true - y_pred) / (math.sqrt((x1 - x2) ** 2 + (y1 - y2) ** 2) * 68)\n",
    "\n",
    "# Calculate the mean NME across all images in the test set\n",
    "mean_NME = 100 * summ / np.shape(pred)[0]\n",
    "print(mean_NME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c06f6cf-b4a7-473b-a27c-5e5bb533e5df",
   "metadata": {},
   "outputs": [],
   "source": [
    "summ = []\n",
    "for i in range(np.shape(pred)[0]):\n",
    "    y_pred = pred[i] * 768\n",
    "    y_true = ts_ary[i]\n",
    "\n",
    "    # Calculate width and height for normalization\n",
    "    w = max(y_true[0:68]) - min(y_true[0:68])\n",
    "    h = max(y_true[68:]) - min(y_true[68:])\n",
    "\n",
    "    # Normalize factor\n",
    "    ni = 2 / (w + h)\n",
    "\n",
    "    # Calculate a similarity metric and append it to the 'summ' list\n",
    "    summ.append(math.sqrt((np.sum(y_true ** 2 + y_pred ** 2)) / 136) * ni)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0427acc4-eae6-4f90-bbd8-d50874dadd0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a histogram of the 'summ' values with 10 bins\n",
    "count, bins_count = np.histogram(summ, bins = 10)\n",
    "\n",
    "# Calculate the Probability Density Function (PDF) using the histogram count values\n",
    "pdf = count / sum(count)\n",
    "\n",
    "# Calculate the Cumulative Distribution Function (CDF) using the PDF values\n",
    "cdf = np.cumsum(pdf)\n",
    "\n",
    "# Plot the CDF using matplotlib\n",
    "plt.plot(bins_count[1:], cdf, label = \"CDF\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "poorya kernel",
   "language": "python",
   "name": "poorya"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
