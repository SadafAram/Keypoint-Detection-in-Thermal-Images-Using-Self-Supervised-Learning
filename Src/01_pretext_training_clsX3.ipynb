{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "deaafd35",
   "metadata": {},
   "source": [
    "## test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f0be4a9-6673-4419-a0b2-3cc8670413e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting the seed value for random number generation\n",
    "seed_value = 42\n",
    "\n",
    "# Setting seed for the random module in Python\n",
    "import random\n",
    "\n",
    "random.seed(seed_value)\n",
    "\n",
    "# Setting seed for random number generation in NumPy\n",
    "import numpy as np\n",
    "\n",
    "np.random.seed(seed_value)\n",
    "\n",
    "# Setting seed for TensorFlow's random number generation\n",
    "import tensorflow as tf\n",
    "\n",
    "tf.random.set_seed(seed_value)\n",
    "\n",
    "# Setting seed for random number generation in Keras (TensorFlow's high-level API)\n",
    "tf.keras.utils.set_random_seed(seed_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c383b35",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Set the environment variable to restrict TensorFlow to use only the first GPU (index 0)\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "# Print the TensorFlow version\n",
    "print(tf.__version__)\n",
    "\n",
    "# List available physical GPU devices for TensorFlow\n",
    "print(tf.config.list_physical_devices('GPU'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "390dae26",
   "metadata": {},
   "source": [
    "## import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "606e4c68",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras.preprocessing import image\n",
    "from tensorflow.keras.layers import Input, Dense, Dropout, GlobalAveragePooling2D, Convolution2D, Flatten, Resizing\n",
    "from tensorflow.keras.layers import MaxPooling2D, AveragePooling2D, BatchNormalization, Conv2DTranspose, concatenate\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau\n",
    "from tensorflow.keras.models import load_model, Model\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from livelossplot.inputs.tf_keras import PlotLossesCallback"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5acd9e83-a0c3-471e-8454-df7976231acb",
   "metadata": {},
   "source": [
    "## lable preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0bdba84-9163-478c-83e4-b24d28547921",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read CSV files using Pandas (file names)\n",
    "tr_df = pd.read_csv('F:/poorya/datasets/keypoint dataset/new pretext dataset/train_aug.csv')\n",
    "vl_df = pd.read_csv('F:/poorya/datasets/keypoint dataset/new pretext dataset/val.csv')\n",
    "ts_df = pd.read_csv('F:/poorya/datasets/keypoint dataset/new pretext dataset/test.csv')\n",
    "\n",
    "# Load NumPy arrays using numpy.load() (labels)\n",
    "tr_ary = np.load('F:/poorya/datasets/keypoint dataset/new pretext dataset/train_aug.npy')\n",
    "ts_ary = np.load('F:/poorya/datasets/keypoint dataset/new pretext dataset/test.npy')\n",
    "vl_ary = np.load('F:/poorya/datasets/keypoint dataset/new pretext dataset/val.npy')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4398d48f",
   "metadata": {},
   "source": [
    "## functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f46b17a-d75d-4da3-b970-64f431326056",
   "metadata": {},
   "outputs": [],
   "source": [
    "sub = []  # Initialize an empty list to store subject identifiers\n",
    "my_subjects = {}  # Initialize an empty dictionary to map subject IDs to a counter\n",
    "counter = 0  # Initialize a counter\n",
    "\n",
    "# List files in the specified directory\n",
    "files = os.listdir('F:/poorya/datasets/keypoint dataset/new pretext dataset/train_aug/')\n",
    "\n",
    "# Extract three-digit subject identifiers from file names and store them in the 'sub' list\n",
    "for i in files:\n",
    "    sub.append(i[-7:-4])\n",
    "\n",
    "# Iterate through a range of numbers up to 95\n",
    "for i in range(95):\n",
    "    # Check if the zero-padded string representation of the number exists in the 'sub' list\n",
    "    if str(i).zfill(3) in sub:\n",
    "        # Map the subject identifier to a counter value in the 'my_subjects' dictionary\n",
    "        my_subjects[str(i).zfill(3)] = counter\n",
    "        counter = counter + 1  # Increment the counter for the next subject"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8c537a3-575d-4d30-872e-398f181b0a54",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataGenerator(tf.keras.utils.Sequence):\n",
    "    '''Generates data for the model'''\n",
    "\n",
    "    def __init__(self, folder, df, ary, batch_size, my_subjects):\n",
    "        '''Initialization'''\n",
    "        self.folder = folder\n",
    "        self.n = df['images'].to_list()\n",
    "        self.ary = ary\n",
    "        self.batch_size = batch_size\n",
    "        self.my_subjects = my_subjects\n",
    "        self.on_epoch_end()\n",
    "\n",
    "    def __len__(self):\n",
    "        '''Denotes the number of batches per epoch'''\n",
    "        return int(np.floor(len(self.n) / self.batch_size))\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        '''Generate one batch of data'''\n",
    "        indexes = self.indexes[index * self.batch_size:(index + 1) * self.batch_size]\n",
    "        list_IDs_temp = [self.n[k] for k in indexes]\n",
    "        x, y = self.__data_generation(list_IDs_temp, indexes)\n",
    "        return x, y\n",
    "\n",
    "    def on_epoch_end(self):\n",
    "        '''Updates indexes after each epoch'''\n",
    "        self.indexes = np.arange(len(self.n))\n",
    "\n",
    "    def __data_generation(self, list_IDs_temp, myindexes):\n",
    "        '''Generates data containing batch_size samples'''\n",
    "        images = np.empty((self.batch_size, 768, 768, 1))\n",
    "        rotation = np.empty((self.batch_size), dtype=int)\n",
    "        subject = np.empty((self.batch_size), dtype=int)\n",
    "        keypoint = np.empty((self.batch_size, 10), dtype=float)\n",
    "\n",
    "        for i, ID in enumerate(list_IDs_temp):\n",
    "            train_img_2 = image.img_to_array(image.load_img(self.folder + ID, color_mode=\"grayscale\"))\n",
    "            train_img_2 /= 255.0\n",
    "            images[i,] = train_img_2\n",
    "\n",
    "            rotation[i] = int(ID[-9])\n",
    "            subject[i] = int(self.my_subjects[ID[-7:-4]])\n",
    "            keypoint[i] = self.ary[int(myindexes[i]), :] / 768\n",
    "\n",
    "        return images, [\n",
    "            to_categorical(subject, num_classes=89),\n",
    "            to_categorical(rotation, num_classes=4),\n",
    "            keypoint\n",
    "        ]\n",
    "\n",
    "\n",
    "bs = 8  # Define batch size\n",
    "\n",
    "# Create instances of DataGenerator for training, validation, and testing data\n",
    "train_gen = DataGenerator('F:/poorya/datasets/keypoint dataset/new pretext dataset/train_aug/', tr_df, tr_ary, bs, my_subjects)\n",
    "val_gen = DataGenerator('F:/poorya/datasets/keypoint dataset/new pretext dataset/val/', vl_df, vl_ary, bs, my_subjects)\n",
    "test_gen = DataGenerator('F:/poorya/datasets/keypoint dataset/new pretext dataset/test/', ts_df, ts_ary, bs, my_subjects)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b13ca300",
   "metadata": {},
   "source": [
    "## model structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b2ba326",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def vgg_block(layer_in, n_filters, n_conv):\n",
    "    '''\n",
    "    Create a VGG block with multiple convolutional layers followed by pooling and normalization.\n",
    "\n",
    "    Arguments:\n",
    "    layer_in: Input layer to the VGG block.\n",
    "    n_filters: Number of filters for the convolutional layers.\n",
    "    n_conv: Number of convolutional layers to be added.\n",
    "\n",
    "    Returns:\n",
    "    layer_in: Output of the VGG block.\n",
    "    '''\n",
    "    for _ in range(n_conv):\n",
    "        layer_in = Convolution2D(n_filters, (3, 3), padding = 'same', activation = 'relu')(layer_in)\n",
    "    layer_in = AveragePooling2D((2, 2), strides = (2, 2))(layer_in)\n",
    "    layer_in = BatchNormalization()(layer_in)\n",
    "    return layer_in\n",
    "\n",
    "\n",
    "def get_model():\n",
    "    '''\n",
    "    Create a deep learning model for prediction.\n",
    "\n",
    "    Returns:\n",
    "    model: A Keras model with specified architecture for multi-task prediction.\n",
    "    '''\n",
    "    # Initialization section\n",
    "    loss_list = {'subject': 'categorical_crossentropy', 'rotation': 'categorical_crossentropy', 'keypoint': 'mse'}\n",
    "    test_metrics = {'subject': 'accuracy', 'rotation': 'accuracy', 'keypoint': tf.keras.metrics.MeanAbsolutePercentageError()}\n",
    "    opt = tf.keras.optimizers.Adamax(learning_rate = 0.0001)\n",
    "    drp = 0.0\n",
    "    act = 'relu'\n",
    "    eact = 'softmax'\n",
    "\n",
    "    # Backbone of the model\n",
    "    model_input = Input(shape = (768, 768, 1))\n",
    "\n",
    "    # Build VGG-style blocks\n",
    "    layer = vgg_block(model_input, 8, 2)\n",
    "    layer = vgg_block(layer, 16, 2)\n",
    "    layer = vgg_block(layer, 32, 2)\n",
    "    layer = vgg_block(layer, 64, 2)\n",
    "    layer = vgg_block(layer, 128, 2)\n",
    "    layer = vgg_block(layer, 256, 2)\n",
    "    layer = vgg_block(layer, 512, 2)\n",
    "\n",
    "    # Flatten the features\n",
    "    xx = Flatten()(layer)\n",
    "\n",
    "    # Subject prediction\n",
    "    x = Dense(512, activation = act)(xx)\n",
    "    x = Dropout(drp)(x)\n",
    "    y1 = Dense(256, activation = act)(x)\n",
    "    y1 = Dropout(drp)(y1)\n",
    "    y1 = Dense(89, activation = eact, name = 'subject')(y1)\n",
    "\n",
    "    # Rotation prediction\n",
    "    y2 = Dense(256, activation = act)(x)\n",
    "    y2 = Dropout(drp)(y2)\n",
    "    y2 = Dense(4, activation = eact, name = 'rotation')(y2)\n",
    "\n",
    "    # Keypoint prediction\n",
    "    y3 = Dense(1024, activation = act)(xx)\n",
    "    y3 = Dropout(drp)(y3)\n",
    "    # ... additional layers for keypoint prediction ...\n",
    "    y3 = Dense(10, name = 'keypoint')(y3)\n",
    "\n",
    "    # Create the model and compile it\n",
    "    model = Model(inputs = model_input, outputs = [y1, y2, y3])\n",
    "    model.compile(loss = loss_list, optimizer = opt, metrics = test_metrics)\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "# Create the model using the defined function\n",
    "model = get_model()\n",
    "# Display a summary of the model architecture\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25629e32",
   "metadata": {},
   "source": [
    "## train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8671ece8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def tr():\n",
    "    '''\n",
    "    Define callbacks for model training.\n",
    "\n",
    "    Returns:\n",
    "    list: A list containing instances of ModelCheckpoint, EarlyStopping, ReduceLROnPlateau, and PlotLossesCallback.\n",
    "    '''\n",
    "    main_chk = ModelCheckpoint(filepath = 'checkpoint', monitor = 'val_keypoint_loss', mode = 'min', verbose = 1, save_best_only = True)\n",
    "    early_st = EarlyStopping(monitor = \"val_loss\", patience = 30, verbose = 1, mode = \"min\")\n",
    "    rduce_lr = ReduceLROnPlateau(monitor = \"val_loss\", factor = 0.8, patience = 5, verbose = 1, mode = \"min\", min_lr = 0.00001)\n",
    "    plot_tr = PlotLossesCallback()\n",
    "    return [main_chk, rduce_lr, plot_tr]\n",
    "\n",
    "\n",
    "# Get the list of callbacks\n",
    "cll = tr()\n",
    "\n",
    "# Fit the model using the defined callbacks and training data\n",
    "history = model.fit(\n",
    "    train_gen,\n",
    "    validation_data = val_gen,\n",
    "    batch_size = bs,\n",
    "    epochs = 300,\n",
    "    verbose = 1,\n",
    "    callbacks = cll,\n",
    "    steps_per_epoch = len(tr_df) // bs,\n",
    "    validation_steps = len(vl_df) // bs\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03900c3a",
   "metadata": {},
   "source": [
    "## evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54c778b7-a76a-445b-bd8c-7222206c4c78",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an empty image array of a specific size\n",
    "img = np.zeros((1, 768, 768, 1)).astype('double')\n",
    "\n",
    "# Load an image for testing\n",
    "train_igg = image.load_img('F:/poorya/datasets/keypoint dataset/new pretext dataset/test/' + ts_df['images'][20], color_mode=\"grayscale\")\n",
    "\n",
    "# Convert the loaded image to an array and normalize the pixel values\n",
    "train_img = image.img_to_array(train_igg)\n",
    "train_img /= 255.0\n",
    "\n",
    "# Assign the loaded and processed image to the empty image array\n",
    "img[0, :, :, :] = train_img\n",
    "img = np.array(img)\n",
    "\n",
    "# Load the trained model for prediction\n",
    "classifier = load_model('checkpoint')\n",
    "\n",
    "# Perform prediction using the model on the test image\n",
    "subj, rott, kpoint = classifier.predict(img)\n",
    "\n",
    "# Extract x and y coordinates from the predicted keypoint\n",
    "x = kpoint[0, 0:5] * 768\n",
    "y = kpoint[0, 5:10] * 768\n",
    "\n",
    "# Visualize the test image and predicted keypoints\n",
    "plt.figure(figsize=(20, 10))\n",
    "plt.imshow(img[0], cmap='gray')\n",
    "plt.scatter(x, y)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8beb313",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Load the trained model for testing\n",
    "testmodel = load_model('checkpoint')\n",
    "\n",
    "# Evaluate the model's performance on the test data\n",
    "# This calculates the loss and accuracy of the model\n",
    "tst_loss, tst_acc = testmodel.evaluate(test_gen, steps=len(ts_df) // bs)"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Classification parameters"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "742a3d10f8170f26"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29eb1e89-c5b2-4caa-a64c-2edddb6fb72c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize arrays to store image data, rotation, and subject labels\n",
    "images = np.empty((len(ts_df), 768, 768, 1))\n",
    "rotation = np.empty((len(ts_df)), dtype = int)\n",
    "subject = np.empty((len(ts_df)), dtype = int)\n",
    "\n",
    "# Loop through the test dataset\n",
    "for i in range(len(ts_df)):\n",
    "    # Load each test image, convert it to an array, and normalize pixel values\n",
    "    train_igg = image.load_img('F:/poorya/datasets/keypoint dataset/new pretext dataset/test/' + ts_df['images'][\n",
    "        i], color_mode = \"grayscale\")\n",
    "    train_img = image.img_to_array(train_igg)\n",
    "    train_img /= 255.0\n",
    "    images[i, :, :, :] = train_img  # Store the preprocessed image\n",
    "\n",
    "    # Extract rotation label from the image filename and convert it to an integer\n",
    "    rotation[i] = int(ts_df['images'][i][-9])\n",
    "\n",
    "    # Extract subject label from the image filename and convert it to an integer\n",
    "    subject[i] = int(my_subjects[ts_df['images'][i][-7:-4]])\n",
    "\n",
    "# Convert the 'images' array to x_test for prediction\n",
    "x_test = np.array(images)\n",
    "\n",
    "# Make predictions using the loaded test model\n",
    "# This predicts rotation, subject, and keypoints\n",
    "predictions = testmodel.predict(x_test, batch_size = 1, verbose = 1, steps = len(ts_df) // 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e664ed40-fef2-4ead-bb08-196d696373e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay, multilabel_confusion_matrix\n",
    "\n",
    "\n",
    "def change(x):\n",
    "    '''\n",
    "    Determine the most probable class index for each sample in the prediction.\n",
    "\n",
    "    Arguments:\n",
    "    x (numpy.ndarray): Array containing the model's predicted probabilities for each class.\n",
    "\n",
    "    Returns:\n",
    "    numpy.ndarray: An array containing the index of the most probable class for each sample.\n",
    "    '''\n",
    "    answer = np.zeros((np.shape(x)[0]), dtype = int)\n",
    "    for i in range(np.shape(x)[0]):\n",
    "        max_value = max(x[i, :])\n",
    "        max_index = list(x[i, :]).index(max_value)\n",
    "        answer[i] = max_index\n",
    "    return answer\n",
    "\n",
    "\n",
    "# Define labels for rotation and subject classes\n",
    "labels = [['0 degrees', '90 degrees', '180 degrees', '270 degrees'], [str(k + 1) for k in range(89)]]\n",
    "\n",
    "# Visualize confusion matrix for 'subject' prediction\n",
    "cm = confusion_matrix(subject, change(predictions[1]))\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix = cm, display_labels = labels[1])\n",
    "disp.plot(cmap = plt.cm.Blues)\n",
    "plt.show()\n",
    "\n",
    "# Visualize confusion matrix for 'rotation' prediction\n",
    "fig, ax = plt.subplots(figsize = (20, 20))\n",
    "plt.rcParams.update({'font.size': 8})\n",
    "cm = confusion_matrix(rotation, change(predictions[0]))\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix = cm, display_labels = labels[0])\n",
    "disp.plot(cmap = plt.cm.Blues, ax = ax)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f34befde-a633-436b-aecb-411d782c4e42",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate and visualize confusion matrix for 'rotation' and predicted 'subject'\n",
    "cm = confusion_matrix(rotation, change(predictions[1]))\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix = cm, display_labels = labels[0])\n",
    "disp.plot(cmap = plt.cm.Blues)\n",
    "plt.show()\n",
    "\n",
    "# Compare the true and predicted 'subject' classes with appended values\n",
    "sub_pred = np.append(change(predictions[0]), [73, 78])\n",
    "sub_true = np.append(subject, [73, 78])\n",
    "\n",
    "# Plot the confusion matrix for 'subject' with appended values\n",
    "fig, ax = plt.subplots(figsize = (20, 20))\n",
    "plt.rcParams.update({'font.size': 8})\n",
    "cm = confusion_matrix(sub_true, sub_pred)\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix = cm, display_labels = labels[1])\n",
    "disp.plot(cmap = plt.cm.Blues, ax = ax)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "poorya kernel",
   "language": "python",
   "name": "poorya"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
